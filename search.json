[
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/002 ethan_bpost/epost_test.html",
    "href": "posts/002 ethan_bpost/epost_test.html",
    "title": "A test post",
    "section": "",
    "text": "Headline\n\n# Stage the change"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethan and LLMs",
    "section": "",
    "text": "Can ChatGPT Crack Crosswords? Evaluating AI‚Äôs Linguistic and Logical Skills\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlinguistics\n\n\npuzzles\n\n\n\nThis post explores ChatGPT‚Äôs ability to solve crossword puzzles by analyzing its linguistic reasoning, word association skills, and pattern recognition. We start with simple clues to establish a baseline, then introduce more complex puzzles to test AI‚Äôs ability to infer word meanings, recognize themes, and leverage contextual hints. Our findings reveal key strengths and limitations in ChatGPT‚Äôs approach to crossword-solving.\n\n\n\n\n\nFeb 8, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\n\n\n\n\n\nCan ChatGPT Master Scrabble? Testing AI‚Äôs Spatial and Logical Reasoning\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nThis post explores ChatGPT‚Äôs ability to solve Scrabble boards by analyzing its spatial reasoning, word-forming logic, and strategy execution. We start with a simple board to establish a baseline, then increase complexity to test AI‚Äôs ability to optimize moves, recognize constraints, and use premium tiles effectively. Our findings reveal key strengths and limitations in ChatGPT‚Äôs approach to word games.\n\n\n\n\n\nFeb 5, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/001_ethan_bpost/bpost_1.html",
    "href": "posts/001_ethan_bpost/bpost_1.html",
    "title": "Can ChatGPT Master Scrabble? Testing AI‚Äôs Spatial and Logical Reasoning",
    "section": "",
    "text": "Scrabble Board Analysis: Testing ChatGPT‚Äôs Spatial Awareness and Letter Processing\nThe goal of this post is to test ChatGPT‚Äôs ability to solve Scrabble boards by analyzing the spatial layout of letters and available moves. We aim to assess how well ChatGPT can interpret images, recognize patterns, and generate optimal Scrabble moves based on board state.\nTo begin, we are starting with a relatively easy board where an optimal move is clearly available. This will allow us to gauge how well ChatGPT can recognize letter placements and suggest a high-scoring word.\nOnce we establish a baseline, we will pivot to a more complex board, increasing the difficulty to further evaluate ChatGPT‚Äôs spatial reasoning and text recognition capabilities in Scrabble.\nBy conducting this experiment, we hope to understand ChatGPT‚Äôs strengths and limitations in processing letter-based games and interpreting board layouts from images.\n\n\n\nCurrent Scrabble Board:\n\nChatGPT will analyze the board and suggest the best possible move using the given letter tiles.\n\n\nI'm sorry for any confusion, but as a text-based AI, I can't view images or screenshots. However, I can help you determine the best possible move if you provide details about the current state of the Scrabble board. Please describe the board position, including any existing words, premium squares, and your current letter rack. With that information, I'll be able to help you find the best move!\n\n\n\n\nScrabble Board Analysis: ChatGPT and Image Processing Limitations\nDuring our initial test, we attempted to use a robocall approach to send an image of a Scrabble board to ChatGPT for analysis. However, we realized that the model does not support image-based processing via API calls. This means that ChatGPT, when accessed through the OpenAI API, cannot directly interpret images unless we extract text manually.\n\n\n\nWorkaround: Directly Uploading to ChatGPT\nSince the robocall method was unsuccessful, we pivoted and uploaded the Scrabble board directly into ChatGPT‚Äôs interface. This allowed the model to process the image and generate an optimal move.\nHere is the response generated by ChatGPT after analyzing the board:\n\nAs seen above, ChatGPT correctly identified the best possible move:\n### Move: Place ‚ÄúLI‚Äù at (Column P, Row 10)\n\nWord Formed: ‚ÄúLI‚Äù\n\nPlacement: The ‚ÄúL‚Äù at (P, 10) and ‚ÄúI‚Äù at (P, 11).\n\nPoints: This move extends an existing word while maximizing efficiency.\n\n\n\n\nA Reminder: This is an Easy Case\nWhile this response is correct, it‚Äôs important to note that this was a relatively simple board with only two tiles available. Our next test will involve a more complex board to further assess ChatGPT‚Äôs spatial reasoning, move optimization, and board evaluation skills.\n\n\nScrabble Board Analysis: Testing ChatGPT‚Äôs Spatial Awareness (Harder Board)\nIn our first test, we evaluated ChatGPT‚Äôs ability to analyze a Scrabble board and generate the best possible move. The first board was relatively simple, and ChatGPT provided a correct response.\nNow, we are increasing the difficulty by presenting a more complex Scrabble board to assess ChatGPT‚Äôs spatial awareness and board evaluation capabilities. The scrabble board is shown below.\n\n\n\n\nChatGPT‚Äôs Suggested Move (Incorrect)\nAfter analyzing the new Scrabble board, ChatGPT generated the following response:\n\nAt first glance, ChatGPT‚Äôs move appears valid, as it successfully connects letters to form a word. However, upon closer inspection, ChatGPT made a fundamental spatial mistake:\n\nMistake: ChatGPT assumed it could place a word downwards from ‚ÄúHUMVEE‚Äù.\n\nReality: ‚ÄúHUMVEE‚Äù is at the bottom row, meaning no tiles can be placed below it.\n\nThis error highlights a major limitation in ChatGPT‚Äôs ability to fully process board constraints. While the model can recognize words and create new ones using existing letters, it struggles with spatial awareness and positional limitations‚Äîwhich are crucial for a game like Scrabble.\n\n\n\nKey Takeaways\n\nChatGPT understands letter placements but struggles with physical constraints on a board.\n\nIt can identify valid words using existing letters but may attempt moves that are spatially impossible.\n\nFuture improvements in AI‚Äôs spatial reasoning could allow for better board-game problem-solving.\n\n\n\nConclusion: Evaluating ChatGPT‚Äôs Scrabble Performance\nThrough this experiment, we tested ChatGPT‚Äôs ability to analyze Scrabble boards, generate optimal moves, and demonstrate spatial awareness. By presenting both an easy board and a more complex board, we identified several key strengths and limitations in how ChatGPT processes Scrabble gameplay.\n\n\n\nFindings: Strengths and Weaknesses\n‚úÖ Strengths: 1. Recognizes Existing Words on the Board: ChatGPT successfully identified letters on the board and could form new words using them.\n2. Generates Valid Words Using Available Tiles: It was able to construct playable words based on the given tile set.\n3. Acknowledges Special Tiles (Triple/Double Score Areas): The model noted the presence of premium scoring tiles (Triple Word Score, Double Letter Score) but did not optimize for them.\n‚ùå Weaknesses: 1. Struggles with Spatial Awareness: In the second test, ChatGPT attempted to place a word downward when the board did not allow it, failing to recognize that ‚ÄúHUMVEE‚Äù was at the bottom row.\n2. Does Not Maximize Score Potential: Although ChatGPT recognized the presence of premium tiles, it did not strategically place words to take full advantage of them. It defaulted to simple word placement rather than high-scoring strategies.\n3. Cannot Calculate Exact Point Values: Unlike human Scrabble players who can calculate score efficiency, ChatGPT did not attempt to maximize points per move based on tile bonuses.\n\n\n\nFinal Takeaway\nChatGPT has a solid grasp of Scrabble fundamentals, particularly in word formation and board recognition. However, it lacks true strategic depth, particularly in leveraging high-scoring word placements and understanding board constraints.\nWhile future AI improvements may address spatial limitations and strategic play, for now, human intuition and Scrabble expertise still outperform AI in high-level gameplay."
  },
  {
    "objectID": "posts/002 ethan_bpost/bpost_2.html",
    "href": "posts/002 ethan_bpost/bpost_2.html",
    "title": "Can ChatGPT Crack Crosswords? Evaluating AI‚Äôs Linguistic and Logical Skills",
    "section": "",
    "text": "To test ChatGPT‚Äôs ability to solve crossword puzzles, I gave it the New York Times mini crossword and analyzed how it approached the clues. The goal was to see how well ChatGPT could infer words based on definitions and whether it could adjust when errors were identified. I also wanted to test if Chatgpt could understand the interactions between different words in columns and rows that intersect.\n\n\n\nBelow is the crossword we are using for this test. It is Superbowl-themed (GO BIRDS) and over my past couple of years of doing the mini every day - I would say today is one of the easier ones. If I had to give it a Roytman-certified difficulty level it would be a 3/10.\n\nChatGPT initially provided the following answers:\n\n1A: CAT ‚úÖ (Correct)\n4A: CHIEF ‚úÖ (Correct)\n6A: LAMAR ‚úÖ (Correct)\n7A: FOE ‚ùå (Incorrect)\n8A: TDS ‚úÖ (Correct)\n1D: CHANT ‚úÖ (Correct)\n2D: THREW ‚úÖ (Correct)\n3D: CASTS ‚úÖ (Correct)\n4D: CLE ‚úÖ (Correct)\n5D: FRY ‚úÖ (Correct)\n\nThe issue was with 7 Across (Opposing side), where ChatGPT suggested FOE, which didn‚Äôt fit correctly with 5 Down (Cook in the pan ‚Üí FRY).\n\n\n\n\nAfter ChatGPT messed up I attempted to fix its mistake, giving it the following prompt: ‚ÄúTake into account the fact that the interactions between the two words. For example, Foe cant be 7a cause the row is 5 letters and the other letters in that word dont match. Solve it in regards to that not just the clue hints.‚Äù\nWhile Chat GPT did correctly replace FOE with a 5-letter word, it still did not align correctly with the ending letter of FRY.\nHere‚Äôs ChatGPT‚Äôs incorrect revision:\n\n\n\n\n\nOnce I highlighted that the revised answer still didn‚Äôt match, ChatGPT finally adjusted 7 Across to ENEMY, which correctly fit the crossword constraints. I specifically pointed it out by prompting it with ‚Äúthe L in rival does not match with the Y in fry‚Äù. Chat GPT‚Äôs final correct response is below:\n\nIt seems that if I point out a specific letter mismatch, chat GPT is finally able to solve it. However, it is important to note that this is Chat GPT‚Äôs third attempt - meaning that through trial and error ‚Äúenemy‚Äù may have been the only possible option remaining.\n\n\n\n\n\nStrengths:\n\nChatGPT correctly inferred most words based on the clues.\n\nIt recognized word associations effectively (e.g., ‚ÄúBengal or lion‚Äù ‚Üí ‚ÄúCAT‚Äù).\n\nIt could adapt and revise answers based on feedback.\n\nWeaknesses:\n\nChatGPT initially failed to check if its answers aligned across the grid.\n\nEven after a revision, it still made a mistake by choosing a word of incorrect length.\n\nHuman intervention was required to get to the final correct solution, which may have resulted from trial and error.\n\n\nOverall, ChatGPT demonstrated strong linguistic reasoning and clue interpretation but struggled with spatial constraints, showing that while AI can assist in solving crosswords, human verification is still necessary."
  },
  {
    "objectID": "posts/002 ethan_bpost/bpost_2.html#can-chatgpt-solve-the-mini-a-step-by-step-analysis",
    "href": "posts/002 ethan_bpost/bpost_2.html#can-chatgpt-solve-the-mini-a-step-by-step-analysis",
    "title": "Can ChatGPT Crack Crosswords? Evaluating AI‚Äôs Linguistic and Logical Skills",
    "section": "",
    "text": "To test ChatGPT‚Äôs ability to solve crossword puzzles, I gave it the New York Times mini crossword and analyzed how it approached the clues. The goal was to see how well ChatGPT could infer words based on definitions and whether it could adjust when errors were identified. I also wanted to test if Chatgpt could understand the interactions between different words in columns and rows that intersect.\n\n\n\nBelow is the crossword we are using for this test. It is Superbowl-themed (GO BIRDS) and over my past couple of years of doing the mini every day - I would say today is one of the easier ones. If I had to give it a Roytman-certified difficulty level it would be a 3/10.\n\nChatGPT initially provided the following answers:\n\n1A: CAT ‚úÖ (Correct)\n4A: CHIEF ‚úÖ (Correct)\n6A: LAMAR ‚úÖ (Correct)\n7A: FOE ‚ùå (Incorrect)\n8A: TDS ‚úÖ (Correct)\n1D: CHANT ‚úÖ (Correct)\n2D: THREW ‚úÖ (Correct)\n3D: CASTS ‚úÖ (Correct)\n4D: CLE ‚úÖ (Correct)\n5D: FRY ‚úÖ (Correct)\n\nThe issue was with 7 Across (Opposing side), where ChatGPT suggested FOE, which didn‚Äôt fit correctly with 5 Down (Cook in the pan ‚Üí FRY).\n\n\n\n\nAfter ChatGPT messed up I attempted to fix its mistake, giving it the following prompt: ‚ÄúTake into account the fact that the interactions between the two words. For example, Foe cant be 7a cause the row is 5 letters and the other letters in that word dont match. Solve it in regards to that not just the clue hints.‚Äù\nWhile Chat GPT did correctly replace FOE with a 5-letter word, it still did not align correctly with the ending letter of FRY.\nHere‚Äôs ChatGPT‚Äôs incorrect revision:\n\n\n\n\n\nOnce I highlighted that the revised answer still didn‚Äôt match, ChatGPT finally adjusted 7 Across to ENEMY, which correctly fit the crossword constraints. I specifically pointed it out by prompting it with ‚Äúthe L in rival does not match with the Y in fry‚Äù. Chat GPT‚Äôs final correct response is below:\n\nIt seems that if I point out a specific letter mismatch, chat GPT is finally able to solve it. However, it is important to note that this is Chat GPT‚Äôs third attempt - meaning that through trial and error ‚Äúenemy‚Äù may have been the only possible option remaining.\n\n\n\n\n\nStrengths:\n\nChatGPT correctly inferred most words based on the clues.\n\nIt recognized word associations effectively (e.g., ‚ÄúBengal or lion‚Äù ‚Üí ‚ÄúCAT‚Äù).\n\nIt could adapt and revise answers based on feedback.\n\nWeaknesses:\n\nChatGPT initially failed to check if its answers aligned across the grid.\n\nEven after a revision, it still made a mistake by choosing a word of incorrect length.\n\nHuman intervention was required to get to the final correct solution, which may have resulted from trial and error.\n\n\nOverall, ChatGPT demonstrated strong linguistic reasoning and clue interpretation but struggled with spatial constraints, showing that while AI can assist in solving crosswords, human verification is still necessary."
  }
]