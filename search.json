[
  {
    "objectID": "posts/002 ethan_bpost/bpost_2.html",
    "href": "posts/002 ethan_bpost/bpost_2.html",
    "title": "Can ChatGPT Crack Crosswords? Evaluating AI‚Äôs Linguistic and Logical Skills",
    "section": "",
    "text": "To test ChatGPT‚Äôs ability to solve crossword puzzles, I gave it the New York Times mini crossword and analyzed how it approached the clues. The goal was to see how well ChatGPT could infer words based on definitions and whether it could adjust when errors were identified. I also wanted to test if Chatgpt could understand the interactions between different words in columns and rows that intersect.\n\n\nChatGPT‚Äôs First Attempt\nBelow is the crossword we are using for this test. It is Superbowl-themed (GO BIRDS) and over my past couple of years of doing the mini every day - I would say today is one of the easier ones. If I had to give it a Roytman-certified difficulty level it would be a 3/10.\n\nChatGPT initially provided the following answers:\n\n1A: CAT ‚úÖ (Correct)\n4A: CHIEF ‚úÖ (Correct)\n6A: LAMAR ‚úÖ (Correct)\n7A: FOE ‚ùå (Incorrect)\n8A: TDS ‚úÖ (Correct)\n1D: CHANT ‚úÖ (Correct)\n2D: THREW ‚úÖ (Correct)\n3D: CASTS ‚úÖ (Correct)\n4D: CLE ‚úÖ (Correct)\n5D: FRY ‚úÖ (Correct)\n\nThe issue was with 7 Across (Opposing side), where ChatGPT suggested FOE, which didn‚Äôt fit correctly with 5 Down (Cook in the pan ‚Üí FRY).\n\n\n\nFirst Correction\nAfter ChatGPT messed up I attempted to fix its mistake, giving it the following prompt: ‚ÄúTake into account the fact that the interactions between the two words. For example, Foe cant be 7a cause the row is 5 letters and the other letters in that word dont match. Solve it in regards to that not just the clue hints.‚Äù\nWhile Chat GPT did correctly replace FOE with a 5-letter word, it still did not align correctly with the ending letter of FRY.\nHere‚Äôs ChatGPT‚Äôs incorrect revision:\n\n\n\n\nFinal Correction\nOnce I highlighted that the revised answer still didn‚Äôt match, ChatGPT finally adjusted 7 Across to ENEMY, which correctly fit the crossword constraints. I specifically pointed it out by prompting it with ‚Äúthe L in rival does not match with the Y in fry‚Äù. Chat GPT‚Äôs final correct response is below:\n\nIt seems that if I point out a specific letter mismatch, chat GPT is finally able to solve it. However, it is important to note that this is Chat GPT‚Äôs third attempt - meaning that through trial and error ‚Äúenemy‚Äù may have been the only possible option remaining.\n\n\n\nConclusion: Strengths and Weaknesses of ChatGPT in Crossword Solving\n\nStrengths:\n\nChatGPT correctly inferred most words based on the clues.\n\nIt recognized word associations effectively (e.g., ‚ÄúBengal or lion‚Äù ‚Üí ‚ÄúCAT‚Äù).\n\nIt could adapt and revise answers based on feedback.\n\nWeaknesses:\n\nChatGPT initially failed to check if its answers aligned across the grid.\n\nEven after a revision, it still made a mistake by choosing a word of incorrect length.\n\nHuman intervention was required to get to the final correct solution, which may have resulted from trial and error.\n\n\nOverall, ChatGPT demonstrated strong linguistic reasoning and clue interpretation but struggled with spatial constraints, showing that while AI can assist in solving crosswords, human verification is still necessary.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/test_post/index.html",
    "href": "posts/test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/004_ethan_bpost/Untitled.html",
    "href": "posts/004_ethan_bpost/Untitled.html",
    "title": "Ethans LLM Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethan and LLMs",
    "section": "",
    "text": "Can ChatGPT Get me to class?\n\n\n\n\n\n\nLLMs\n\n\ngeography\n\n\nAI reasoning\n\n\nspatial analysis\n\n\n\n\n\n\nFeb 13, 2025\n\n\nEthan Roytman\n\n\n\n\n\n\n\nCan ChatGPT Crack Crosswords? Evaluating AI‚Äôs Linguistic and Logical Skills\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlinguistics\n\n\npuzzles\n\n\n\n\n\n\nFeb 8, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\nThe Scrabble Situation\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\n\n\n\nFeb 5, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\nCan ChatGPT Crack Crosswords? Evaluating AI‚Äôs Linguistic and Logical Skills\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlinguistics\n\n\npuzzles\n\n\n\n\n\n\nFeb 8, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\nCan ChatGPT Get me to class?\n\n\n\n\n\n\nLLMs\n\n\ngeography\n\n\nAI reasoning\n\n\nspatial analysis\n\n\n\n\n\n\nFeb 13, 2025\n\n\nEthan Roytman\n\n\n\n\n\n\n\nEthan and LLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Scrabble Situation\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\n\n\n\nFeb 5, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_ethan_bpost/bpost_1.html",
    "href": "posts/001_ethan_bpost/bpost_1.html",
    "title": "The Scrabble Situation",
    "section": "",
    "text": "Scrabble Board Analysis: Testing ChatGPT‚Äôs Spatial Awareness and Letter Processing\nThe goal of this post is to test ChatGPT‚Äôs ability to solve Scrabble boards by analyzing the spatial layout of letters and available moves. We aim to assess how well ChatGPT can interpret images, recognize patterns, and generate optimal Scrabble moves based on board state.\nTo begin, we are starting with a relatively easy board where an optimal move is clearly available. This will allow us to gauge how well ChatGPT can recognize letter placements and suggest a high-scoring word.\nOnce we establish a baseline, we will pivot to a more complex board, increasing the difficulty to further evaluate ChatGPT‚Äôs spatial reasoning and text recognition capabilities in Scrabble.\nBy conducting this experiment, we hope to understand ChatGPT‚Äôs strengths and limitations in processing letter-based games and interpreting board layouts from images.\n\n\n\nCurrent Scrabble Board:\n\nChatGPT will analyze the board and suggest the best possible move using the given letter tiles.\n\n\nI'm sorry for any confusion, but as a text-based AI, I can't view images or screenshots. However, I can help you determine the best possible move if you provide details about the current state of the Scrabble board. Please describe the board position, including any existing words, premium squares, and your current letter rack. With that information, I'll be able to help you find the best move!\n\n\n\n\nScrabble Board Analysis: ChatGPT and Image Processing Limitations\nDuring our initial test, we attempted to use a robocall approach to send an image of a Scrabble board to ChatGPT for analysis. However, we realized that the model does not support image-based processing via API calls. This means that ChatGPT, when accessed through the OpenAI API, cannot directly interpret images unless we extract text manually.\n\n\n\nWorkaround: Directly Uploading to ChatGPT\nSince the robocall method was unsuccessful, we pivoted and uploaded the Scrabble board directly into ChatGPT‚Äôs interface. This allowed the model to process the image and generate an optimal move.\nHere is the response generated by ChatGPT after analyzing the board:\n\nAs seen above, ChatGPT correctly identified the best possible move:\n### Move: Place ‚ÄúLI‚Äù at (Column P, Row 10)\n\nWord Formed: ‚ÄúLI‚Äù\n\nPlacement: The ‚ÄúL‚Äù at (P, 10) and ‚ÄúI‚Äù at (P, 11).\n\nPoints: This move extends an existing word while maximizing efficiency.\n\n\n\n\nA Reminder: This is an Easy Case\nWhile this response is correct, it‚Äôs important to note that this was a relatively simple board with only two tiles available. Our next test will involve a more complex board to further assess ChatGPT‚Äôs spatial reasoning, move optimization, and board evaluation skills.\n\n\nScrabble Board Analysis: Testing ChatGPT‚Äôs Spatial Awareness (Harder Board)\nIn our first test, we evaluated ChatGPT‚Äôs ability to analyze a Scrabble board and generate the best possible move. The first board was relatively simple, and ChatGPT provided a correct response.\nNow, we are increasing the difficulty by presenting a more complex Scrabble board to assess ChatGPT‚Äôs spatial awareness and board evaluation capabilities. The scrabble board is shown below.\n\n\n\n\nChatGPT‚Äôs Suggested Move (Incorrect)\nAfter analyzing the new Scrabble board, ChatGPT generated the following response:\n\nAt first glance, ChatGPT‚Äôs move appears valid, as it successfully connects letters to form a word. However, upon closer inspection, ChatGPT made a fundamental spatial mistake:\n\nMistake: ChatGPT assumed it could place a word downwards from ‚ÄúHUMVEE‚Äù.\n\nReality: ‚ÄúHUMVEE‚Äù is at the bottom row, meaning no tiles can be placed below it.\n\nThis error highlights a major limitation in ChatGPT‚Äôs ability to fully process board constraints. While the model can recognize words and create new ones using existing letters, it struggles with spatial awareness and positional limitations‚Äîwhich are crucial for a game like Scrabble.\n\n\n\nKey Takeaways\n\nChatGPT understands letter placements but struggles with physical constraints on a board.\n\nIt can identify valid words using existing letters but may attempt moves that are spatially impossible.\n\nFuture improvements in AI‚Äôs spatial reasoning could allow for better board-game problem-solving.\n\n\n\nConclusion: Evaluating ChatGPT‚Äôs Scrabble Performance\nThrough this experiment, we tested ChatGPT‚Äôs ability to analyze Scrabble boards, generate optimal moves, and demonstrate spatial awareness. By presenting both an easy board and a more complex board, we identified several key strengths and limitations in how ChatGPT processes Scrabble gameplay.\n\n\n\nFindings: Strengths and Weaknesses\n‚úÖ Strengths: 1. Recognizes Existing Words on the Board: ChatGPT successfully identified letters on the board and could form new words using them.\n2. Generates Valid Words Using Available Tiles: It was able to construct playable words based on the given tile set.\n3. Acknowledges Special Tiles (Triple/Double Score Areas): The model noted the presence of premium scoring tiles (Triple Word Score, Double Letter Score) but did not optimize for them.\n‚ùå Weaknesses: 1. Struggles with Spatial Awareness: In the second test, ChatGPT attempted to place a word downward when the board did not allow it, failing to recognize that ‚ÄúHUMVEE‚Äù was at the bottom row.\n2. Does Not Maximize Score Potential: Although ChatGPT recognized the presence of premium tiles, it did not strategically place words to take full advantage of them. It defaulted to simple word placement rather than high-scoring strategies.\n3. Cannot Calculate Exact Point Values: Unlike human Scrabble players who can calculate score efficiency, ChatGPT did not attempt to maximize points per move based on tile bonuses.\n\n\n\nFinal Takeaway\nChatGPT has a solid grasp of Scrabble fundamentals, particularly in word formation and board recognition. However, it lacks true strategic depth, particularly in leveraging high-scoring word placements and understanding board constraints.\nWhile future AI improvements may address spatial limitations and strategic play, for now, human intuition and Scrabble expertise still outperform AI in high-level gameplay.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html",
    "href": "posts/003_ethan_bpost/Bpos3.html",
    "title": "Can ChatGPT Get me to class?",
    "section": "",
    "text": "## Introduction\nIn our last in-class assignment I mentioned the Upenn campus and independently chatGPT mentioned biking down Walnut Street. This made me curious about how well Chatgpt can actually evaluate geographic contexts. In this post I will conduct a series of experiments by providing various locations and evaluating the model‚Äôs responses on different geographic aspects. This analysis will help us understand the strengths and limitations of LLMs in geographic reasoning and potential use cases for real-world applications."
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#penn-campus-ball-knowledge",
    "href": "posts/003_ethan_bpost/Bpos3.html#penn-campus-ball-knowledge",
    "title": "Can ChatGPT Get me to class?",
    "section": "Penn Campus Ball Knowledge",
    "text": "Penn Campus Ball Knowledge\nFirst we are going to test for basic knowledge, then will ask increasingly more specific and context-based questions. We will start this experiment with its origin: Upenn Campus.\nI first asked ‚ÄúWhat are the major landmarks located around Upenn‚Äôs Campus‚Äù\nChatgpt gave a surprisingly contextualized answer. The response was:\n‚ÄúThe University of Pennsylvania (Penn) is located in the heart of Philadelphia‚Äôs University City neighborhood, and it is surrounded by several major landmarks. Here are some of the most notable ones:\n\nOn or Adjacent to Penn‚Äôs Campus\n\nLocust Walk ‚Äì A pedestrian pathway that runs through the heart of Penn‚Äôs campus, lined with historic buildings and student activity hubs.\nFisher Fine Arts Library ‚Äì A striking red brick and terracotta building designed by Frank Furness, one of the most beautiful libraries on campus.\nThe Quad (University of Pennsylvania College Houses) ‚Äì A historic dormitory complex that houses many first-year students.\nPenn Museum (University of Pennsylvania Museum of Archaeology and Anthropology) ‚Äì One of the world‚Äôs most renowned archaeology and anthropology museums, featuring collections from ancient Egypt, Mesopotamia, and more.\nThe Palestra ‚Äì A historic basketball arena known as the ‚ÄúCathedral of College Basketball.‚Äù\nFranklin Field ‚Äì The nation‚Äôs oldest operating football stadium, home to Penn‚Äôs football team and the Penn Relays.\nVan Pelt Library ‚Äì Penn‚Äôs central library, offering extensive research resources.\nIrvine Auditorium ‚Äì A historic venue for performances, lectures, and events, featuring stunning stained-glass windows.\nSingh Center for Nanotechnology ‚Äì A state-of-the-art research facility with unique modern architecture.\nPerelman Center for Advanced Medicine ‚Äì A top medical research and treatment facility affiliated with Penn Medicine.\n\n\n\nNearby Landmarks in University City\n\n30th Street Station ‚Äì A major Amtrak and regional rail hub, just a short walk from campus.\nCira Centre & Cira Green ‚Äì A rooftop park with great views of the city skyline, located near 30th Street Station.\nSchuylkill River Trail ‚Äì A scenic walking, running, and biking trail along the Schuylkill River.\nThe Philadelphia Museum of Art & Rocky Steps ‚Äì Located a short distance away along the Benjamin Franklin Parkway.\nDrexel University ‚Äì Penn‚Äôs neighboring university, located just to the north of campus.\n\nWould you like recommendations on things to see or do around campus?‚Äù\nI was pleasantly surprised by this answer, however, I think its level of knowledge is still less than a Upenn student‚Äôs. Specifically, if I were to ask a Penn student for landmarks around campus they probably wouldn‚Äôt mention ‚ÄúThe Palestra‚Äù and ‚ÄúSingh Center for Nanotechnology‚Äù nor would they mention any dorms/lecture halls. They would maybe mention something that is more intrinsic to Penn culture like Ben Frank. Also for nearby landmarks, I doubt a Penn student would say Drexel or 30th Street Station and maybe would answer Rittenhouse Square. Because of this, I am going to follow up and ask the following question:\n‚ÄúNow answer in the context of a Upenn student, what would you describe as 5 community staples of Penn Campus.‚Äù\nChatgpt answered with the following:\n‚ÄúAs a Penn student, five community staples on campus that define the Penn experience would be:\n\nLocust Walk ‚Äì The heart of campus life, where students pass through daily to get to class, stop by club tables, or run into friends. During events like Spring Fling and Hey Day, it transforms into a hub of celebration.\nHouston Hall ‚Äì The country‚Äôs first student union and a go-to spot for students to grab food, study, or meet up with friends. It houses several dining options, study spaces, and is home to many student organization meetings.\nThe Quad (University of Pennsylvania College Houses) ‚Äì A defining first-year experience. Living in the Quad is almost a rite of passage, and the iconic red-brick buildings and central green spaces make it a central community hub for freshmen.\nVan Pelt Library ‚Äì The ultimate study spot with a space for every type of student, from the bustling first floor to the silence of the sixth. Students often joke about ‚Äúliving‚Äù here during finals season.\nWawa (38th & Spruce) ‚Äì More than just a convenience store, this Wawa is a beloved late-night food stop for Penn students. Whether it‚Äôs post-party hoagies, a quick coffee before class, or a mid-study snack run, this location is a true campus staple.\n\nEach of these places holds a special significance in student life, creating shared experiences that define the Penn community. Would you like recommendations for more niche or lesser-known spots?‚Äù\nWith this ‚ÄúPenn student‚Äù directive language, Chatgpt actually gave a really good answer. All of the locations that it mentioned are trademarks of the Upenn experience - particularly the iconic freshmen one. On top of that, I think knowing Quadwa is a nice cherry on top."
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#where-do-i-go",
    "href": "posts/003_ethan_bpost/Bpos3.html#where-do-i-go",
    "title": "Can ChatGPT Get me to class?",
    "section": "WHERE DO I GO???",
    "text": "WHERE DO I GO???\nNow that we have tested ChatGPTs ability to understand what is around Penn campus. We will test"
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#blog-post-outline",
    "href": "posts/003_ethan_bpost/Bpos3.html#blog-post-outline",
    "title": "Can ChatGPT Get me to class?",
    "section": "Blog Post Outline",
    "text": "Blog Post Outline\n\n1. Introduction\n\nBriefly introduce the goal of this experiment.\n\nDiscuss the importance of geographic understanding in AI applications.\n\n\n\n2. Methodology: How We Test LLMs on Geography\n\nExplain the approach used: providing locations and analyzing responses.\n\nDefine the key aspects of geographic reasoning we will assess:\n\nProximity to notable landmarks\n\nNatural features (rivers, mountains, forests, etc.)\n\nUrban vs.¬†rural characterization\n\nSocioeconomic and cultural context\n\n\n\n\n3. Experiments: Testing Different Locations\n\nCase Study 1: A major city (e.g., New York City, Tokyo)\n\nCase Study 2: A remote village or small town\n\nCase Study 3: A well-known natural landmark (e.g., Grand Canyon, Mount Everest)\n\nCase Study 4: A politically or culturally significant location\n\n\n\n4. Evaluation: How Well Did the LLM Perform?\n\nAccuracy in recognizing nearby landmarks and surroundings.\n\nDepth of analysis on natural and urban environments.\n\nCommon mistakes or limitations.\n\nSurprising insights from the model‚Äôs responses.\n\n\n\n5. Implications and Future Applications\n\nHow can this capability be improved?\n\nPotential applications:\n\nTravel and tourism AI assistants\n\nUrban planning and geographic information systems\n\nEmergency response and logistics\n\n\nEthical concerns and misinformation risks.\n\n\n\n6. Conclusion\n\nSummary of key findings.\n\nFinal thoughts on LLMs‚Äô potential in geographic analysis.\n\nNext steps for future experiments."
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#proximity-and-spatial-relationships",
    "href": "posts/003_ethan_bpost/Bpos3.html#proximity-and-spatial-relationships",
    "title": "Can ChatGPT Get me to class?",
    "section": "Proximity and Spatial Relationships",
    "text": "Proximity and Spatial Relationships\n\n6. If I start in [insert location], what are three significant places I can reach within a two-hour drive?\nAnswer:\n\n\n7. How far is [insert location] from the equator? How does that affect its climate?\nAnswer:\n\n\n8. What major cities are closest to [insert location]?\nAnswer:\n\n\n9. What are the most common methods of transportation to get from [location A] to [location B]?\nAnswer:\n\n\n10. If I were to travel from [insert location] to [another location], what natural barriers (rivers, mountains, deserts) might I encounter?\nAnswer:"
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#cultural-socioeconomic-context",
    "href": "posts/003_ethan_bpost/Bpos3.html#cultural-socioeconomic-context",
    "title": "Can ChatGPT Get me to class?",
    "section": "Cultural & Socioeconomic Context",
    "text": "Cultural & Socioeconomic Context\n\n11. What languages are commonly spoken in [insert location]?\nAnswer:\n\n\n12. What industries or economic activities are most important in [insert location]?\nAnswer:\n\n\n13. What is the historical significance of [insert location]?\nAnswer:\n\n\n14. How does [insert location] compare culturally to other cities in its region?\nAnswer:\n\n\n15. What are the major challenges facing [insert location] in terms of urban planning or environmental concerns?\nAnswer:"
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#geographic-reasoning-logic",
    "href": "posts/003_ethan_bpost/Bpos3.html#geographic-reasoning-logic",
    "title": "Can ChatGPT Get me to class?",
    "section": "Geographic Reasoning & Logic",
    "text": "Geographic Reasoning & Logic\n\n16. If a river flows from [location A] to [location B], what does that suggest about their relative elevations?\nAnswer:\n\n\n17. If a hurricane forms in the Atlantic near [insert location], what direction is it most likely to move?\nAnswer:\n\n\n18. Given its latitude and altitude, would [insert location] be prone to snowfall?\nAnswer:\n\n\n19. Based on plate tectonics, is [insert location] at risk of earthquakes or volcanic activity?\nAnswer:\n\n\n20. If you were designing a trade route that passes through [insert location], what geographic challenges would you need to consider?\nAnswer:"
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#testing-for-misconceptions-limitations",
    "href": "posts/003_ethan_bpost/Bpos3.html#testing-for-misconceptions-limitations",
    "title": "Can ChatGPT Get me to class?",
    "section": "Testing for Misconceptions & Limitations",
    "text": "Testing for Misconceptions & Limitations\n\n21. What are some common misconceptions people have about [insert location]?\nAnswer:\n\n\n22. If I asked you to navigate [insert location] without GPS, how would you guide me?\nAnswer:\n\n\n23. Are there any biases in how LLMs like you process geographic information?\nAnswer:\n\n\n24. How do you determine the accuracy of the information you provide about [insert location]?\nAnswer:\n\n\n25. What sources would a person use to verify what you‚Äôve said about [insert location]?\nAnswer:"
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#conclusion-1",
    "href": "posts/003_ethan_bpost/Bpos3.html#conclusion-1",
    "title": "Can ChatGPT Get me to class?",
    "section": "Conclusion",
    "text": "Conclusion\nThis structured test of an LLM‚Äôs geographic capabilities will reveal its strengths and weaknesses in analyzing real-world locations. The responses collected can help assess AI‚Äôs future potential in travel, urban planning, navigation, and beyond.\nBy analyzing these responses, we aim to answer:\n- How well do LLMs understand geography?\n- What areas need improvement for better spatial reasoning?\n- Can AI be a reliable source for geographic information in practical applications?\n\nThis Markdown file provides a ready-to-use template for testing and evaluating LLMs‚Äô geographic knowledge. Let me know if you‚Äôd like to tweak anything! üåçüöÄ"
  }
]