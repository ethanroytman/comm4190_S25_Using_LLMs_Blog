[
  {
    "objectID": "posts/002 ethan_bpost/bpost_2.html",
    "href": "posts/002 ethan_bpost/bpost_2.html",
    "title": "The NYT mini v ChatGPT",
    "section": "",
    "text": "To test ChatGPT‚Äôs ability to solve crossword puzzles, I gave it the New York Times mini crossword and analyzed how it approached the clues. The goal was to see how well ChatGPT could infer words based on definitions and whether it could adjust when errors were identified. I also wanted to test if Chatgpt could understand the interactions between different words in columns and rows that intersect.\n\n\nChatGPT‚Äôs First Attempt\nBelow is the crossword we are using for this test. It is Superbowl-themed (GO BIRDS) and over my past couple of years of doing the mini every day - I would say today is one of the easier ones. If I had to give it a Roytman-certified difficulty level it would be a 3/10.\n\nChatGPT initially provided the following answers:\n\n1A: CAT ‚úÖ (Correct)\n4A: CHIEF ‚úÖ (Correct)\n6A: LAMAR ‚úÖ (Correct)\n7A: FOE ‚ùå (Incorrect)\n8A: TDS ‚úÖ (Correct)\n1D: CHANT ‚úÖ (Correct)\n2D: THREW ‚úÖ (Correct)\n3D: CASTS ‚úÖ (Correct)\n4D: CLE ‚úÖ (Correct)\n5D: FRY ‚úÖ (Correct)\n\nThe issue was with 7 Across (Opposing side), where ChatGPT suggested FOE, which didn‚Äôt fit correctly with 5 Down (Cook in the pan ‚Üí FRY).\n\n\n\nFirst Correction\nAfter ChatGPT messed up I attempted to fix its mistake, giving it the following prompt: ‚ÄúTake into account the fact that the interactions between the two words. For example, Foe cant be 7a cause the row is 5 letters and the other letters in that word dont match. Solve it in regards to that not just the clue hints.‚Äù\nWhile Chat GPT did correctly replace FOE with a 5-letter word, it still did not align correctly with the ending letter of FRY.\nHere‚Äôs ChatGPT‚Äôs incorrect revision:\n\n\n\n\nFinal Correction\nOnce I highlighted that the revised answer still didn‚Äôt match, ChatGPT finally adjusted 7 Across to ENEMY, which correctly fit the crossword constraints. I specifically pointed it out by prompting it with ‚Äúthe L in rival does not match with the Y in fry‚Äù. Chat GPT‚Äôs final correct response is below:\n\nIt seems that if I point out a specific letter mismatch, chat GPT is finally able to solve it. However, it is important to note that this is Chat GPT‚Äôs third attempt - meaning that through trial and error ‚Äúenemy‚Äù may have been the only possible option remaining.\n\n\n\nConclusion: Strengths and Weaknesses of ChatGPT in Crossword Solving\n\nStrengths:\n\nChatGPT correctly inferred most words based on the clues.\n\nIt recognized word associations effectively (e.g., ‚ÄúBengal or lion‚Äù ‚Üí ‚ÄúCAT‚Äù).\n\nIt could adapt and revise answers based on feedback.\n\nWeaknesses:\n\nChatGPT initially failed to check if its answers aligned across the grid.\n\nEven after a revision, it still made a mistake by choosing a word of incorrect length.\n\nHuman intervention was required to get to the final correct solution, which may have resulted from trial and error.\n\n\nOverall, ChatGPT demonstrated strong linguistic reasoning and clue interpretation but struggled with spatial constraints, showing that while AI can assist in solving crosswords, human verification is still necessary.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/test_post/index.html",
    "href": "posts/test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/004_ethan_bpost/Untitled.html",
    "href": "posts/004_ethan_bpost/Untitled.html",
    "title": "Ethans LLM Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ethan and LLMs",
    "section": "",
    "text": "Can ChatGPT Get me to class?\n\n\n\n\n\n\nLLMs\n\n\ngeography\n\n\nAI reasoning\n\n\nspatial analysis\n\n\n\nThis post examines the ability of large language models to analyze geographic locations, describe their surroundings, and interpret spatial relationships.\n\n\n\n\n\nFeb 13, 2025\n\n\nEthan Roytman\n\n\n\n\n\n\n\nThe NYT mini v ChatGPT\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlinguistics\n\n\npuzzles\n\n\n\nThis post explores ChatGPT‚Äôs ability to solve crossword puzzles by analyzing its linguistic reasoning, word association skills, and pattern recognition.\n\n\n\n\n\nFeb 8, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\nThe Scrabble Situation\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nThis post explores ChatGPT‚Äôs ability to solve Scrabble boards by analyzing its spatial reasoning, word-forming logic, and strategy execution.\n\n\n\n\n\nFeb 5, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\n\n\n\n\n\nEthan and LLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan ChatGPT Get me to class?\n\n\n\n\n\n\nLLMs\n\n\ngeography\n\n\nAI reasoning\n\n\nspatial analysis\n\n\n\nThis post examines the ability of large language models to analyze geographic locations, describe their surroundings, and interpret spatial relationships.\n\n\n\n\n\nFeb 13, 2025\n\n\nEthan Roytman\n\n\n\n\n\n\n\nThe NYT mini v ChatGPT\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlinguistics\n\n\npuzzles\n\n\n\nThis post explores ChatGPT‚Äôs ability to solve crossword puzzles by analyzing its linguistic reasoning, word association skills, and pattern recognition.\n\n\n\n\n\nFeb 8, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\nThe Scrabble Situation\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nThis post explores ChatGPT‚Äôs ability to solve Scrabble boards by analyzing its spatial reasoning, word-forming logic, and strategy execution.\n\n\n\n\n\nFeb 5, 2024\n\n\nEthan Roytman\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_ethan_bpost/bpost_1.html",
    "href": "posts/001_ethan_bpost/bpost_1.html",
    "title": "The Scrabble Situation",
    "section": "",
    "text": "Scrabble Board Analysis: Testing ChatGPT‚Äôs Spatial Awareness and Letter Processing\nThe goal of this post is to test ChatGPT‚Äôs ability to solve Scrabble boards by analyzing the spatial layout of letters and available moves. We aim to assess how well ChatGPT can interpret images, recognize patterns, and generate optimal Scrabble moves based on board state.\nTo begin, we are starting with a relatively easy board where an optimal move is clearly available. This will allow us to gauge how well ChatGPT can recognize letter placements and suggest a high-scoring word.\nOnce we establish a baseline, we will pivot to a more complex board, increasing the difficulty to further evaluate ChatGPT‚Äôs spatial reasoning and text recognition capabilities in Scrabble.\nBy conducting this experiment, we hope to understand ChatGPT‚Äôs strengths and limitations in processing letter-based games and interpreting board layouts from images.\n\n\n\nCurrent Scrabble Board:\n\nChatGPT will analyze the board and suggest the best possible move using the given letter tiles.\n\n\nI'm sorry for any confusion, but as a text-based AI, I can't view images or screenshots. However, I can help you determine the best possible move if you provide details about the current state of the Scrabble board. Please describe the board position, including any existing words, premium squares, and your current letter rack. With that information, I'll be able to help you find the best move!\n\n\n\n\nScrabble Board Analysis: ChatGPT and Image Processing Limitations\nDuring our initial test, we attempted to use a robocall approach to send an image of a Scrabble board to ChatGPT for analysis. However, we realized that the model does not support image-based processing via API calls. This means that ChatGPT, when accessed through the OpenAI API, cannot directly interpret images unless we extract text manually.\n\n\n\nWorkaround: Directly Uploading to ChatGPT\nSince the robocall method was unsuccessful, we pivoted and uploaded the Scrabble board directly into ChatGPT‚Äôs interface. This allowed the model to process the image and generate an optimal move.\nHere is the response generated by ChatGPT after analyzing the board:\n\nAs seen above, ChatGPT correctly identified the best possible move:\n### Move: Place ‚ÄúLI‚Äù at (Column P, Row 10)\n\nWord Formed: ‚ÄúLI‚Äù\n\nPlacement: The ‚ÄúL‚Äù at (P, 10) and ‚ÄúI‚Äù at (P, 11).\n\nPoints: This move extends an existing word while maximizing efficiency.\n\n\n\n\nA Reminder: This is an Easy Case\nWhile this response is correct, it‚Äôs important to note that this was a relatively simple board with only two tiles available. Our next test will involve a more complex board to further assess ChatGPT‚Äôs spatial reasoning, move optimization, and board evaluation skills.\n\n\nScrabble Board Analysis: Testing ChatGPT‚Äôs Spatial Awareness (Harder Board)\nIn our first test, we evaluated ChatGPT‚Äôs ability to analyze a Scrabble board and generate the best possible move. The first board was relatively simple, and ChatGPT provided a correct response.\nNow, we are increasing the difficulty by presenting a more complex Scrabble board to assess ChatGPT‚Äôs spatial awareness and board evaluation capabilities. The scrabble board is shown below.\n\n\n\n\nChatGPT‚Äôs Suggested Move (Incorrect)\nAfter analyzing the new Scrabble board, ChatGPT generated the following response:\n\nAt first glance, ChatGPT‚Äôs move appears valid, as it successfully connects letters to form a word. However, upon closer inspection, ChatGPT made a fundamental spatial mistake:\n\nMistake: ChatGPT assumed it could place a word downwards from ‚ÄúHUMVEE‚Äù.\n\nReality: ‚ÄúHUMVEE‚Äù is at the bottom row, meaning no tiles can be placed below it.\n\nThis error highlights a major limitation in ChatGPT‚Äôs ability to fully process board constraints. While the model can recognize words and create new ones using existing letters, it struggles with spatial awareness and positional limitations‚Äîwhich are crucial for a game like Scrabble.\n\n\n\nKey Takeaways\n\nChatGPT understands letter placements but struggles with physical constraints on a board.\n\nIt can identify valid words using existing letters but may attempt moves that are spatially impossible.\n\nFuture improvements in AI‚Äôs spatial reasoning could allow for better board-game problem-solving.\n\n\n\nConclusion: Evaluating ChatGPT‚Äôs Scrabble Performance\nThrough this experiment, we tested ChatGPT‚Äôs ability to analyze Scrabble boards, generate optimal moves, and demonstrate spatial awareness. By presenting both an easy board and a more complex board, we identified several key strengths and limitations in how ChatGPT processes Scrabble gameplay.\n\n\n\nFindings: Strengths and Weaknesses\n‚úÖ Strengths: 1. Recognizes Existing Words on the Board: ChatGPT successfully identified letters on the board and could form new words using them.\n2. Generates Valid Words Using Available Tiles: It was able to construct playable words based on the given tile set.\n3. Acknowledges Special Tiles (Triple/Double Score Areas): The model noted the presence of premium scoring tiles (Triple Word Score, Double Letter Score) but did not optimize for them.\n‚ùå Weaknesses: 1. Struggles with Spatial Awareness: In the second test, ChatGPT attempted to place a word downward when the board did not allow it, failing to recognize that ‚ÄúHUMVEE‚Äù was at the bottom row.\n2. Does Not Maximize Score Potential: Although ChatGPT recognized the presence of premium tiles, it did not strategically place words to take full advantage of them. It defaulted to simple word placement rather than high-scoring strategies.\n3. Cannot Calculate Exact Point Values: Unlike human Scrabble players who can calculate score efficiency, ChatGPT did not attempt to maximize points per move based on tile bonuses.\n\n\n\nFinal Takeaway\nChatGPT has a solid grasp of Scrabble fundamentals, particularly in word formation and board recognition. However, it lacks true strategic depth, particularly in leveraging high-scoring word placements and understanding board constraints.\nWhile future AI improvements may address spatial limitations and strategic play, for now, human intuition and Scrabble expertise still outperform AI in high-level gameplay.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html",
    "href": "posts/003_ethan_bpost/Bpos3.html",
    "title": "Can ChatGPT Get me to class?",
    "section": "",
    "text": "## Introduction\nIn our last in-class assignment I mentioned the Upenn campus and independently chatGPT mentioned biking down Walnut Street. This made me curious about how well Chatgpt can actually evaluate geographic contexts. In this post I will conduct a series of experiments by providing various locations and evaluating the model‚Äôs responses on different geographic aspects. This analysis will help us understand the strengths and limitations of LLMs in geographic reasoning and potential use cases for real-world applications."
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#penn-campus-ball-knowledge",
    "href": "posts/003_ethan_bpost/Bpos3.html#penn-campus-ball-knowledge",
    "title": "Can ChatGPT Get me to class?",
    "section": "Penn Campus Ball Knowledge",
    "text": "Penn Campus Ball Knowledge\nFirst we are going to test for basic knowledge, then will ask increasingly more specific and context-based questions. We will start this experiment with its origin: Upenn Campus.\nI first asked ‚ÄúWhat are the major landmarks located around Upenn‚Äôs Campus‚Äù\nChatgpt gave a surprisingly contextualized answer. The response was:\n‚ÄúThe University of Pennsylvania (Penn) is located in the heart of Philadelphia‚Äôs University City neighborhood, and it is surrounded by several major landmarks. Here are some of the most notable ones:\n\nOn or Adjacent to Penn‚Äôs Campus\n\nLocust Walk ‚Äì A pedestrian pathway that runs through the heart of Penn‚Äôs campus, lined with historic buildings and student activity hubs.\nFisher Fine Arts Library ‚Äì A striking red brick and terracotta building designed by Frank Furness, one of the most beautiful libraries on campus.\nThe Quad (University of Pennsylvania College Houses) ‚Äì A historic dormitory complex that houses many first-year students.\nPenn Museum (University of Pennsylvania Museum of Archaeology and Anthropology) ‚Äì One of the world‚Äôs most renowned archaeology and anthropology museums, featuring collections from ancient Egypt, Mesopotamia, and more.\nThe Palestra ‚Äì A historic basketball arena known as the ‚ÄúCathedral of College Basketball.‚Äù\nFranklin Field ‚Äì The nation‚Äôs oldest operating football stadium, home to Penn‚Äôs football team and the Penn Relays.\nVan Pelt Library ‚Äì Penn‚Äôs central library, offering extensive research resources.\nIrvine Auditorium ‚Äì A historic venue for performances, lectures, and events, featuring stunning stained-glass windows.\nSingh Center for Nanotechnology ‚Äì A state-of-the-art research facility with unique modern architecture.\nPerelman Center for Advanced Medicine ‚Äì A top medical research and treatment facility affiliated with Penn Medicine.\n\n\n\nNearby Landmarks in University City\n\n30th Street Station ‚Äì A major Amtrak and regional rail hub, just a short walk from campus.\nCira Centre & Cira Green ‚Äì A rooftop park with great views of the city skyline, located near 30th Street Station.\nSchuylkill River Trail ‚Äì A scenic walking, running, and biking trail along the Schuylkill River.\nThe Philadelphia Museum of Art & Rocky Steps ‚Äì Located a short distance away along the Benjamin Franklin Parkway.\nDrexel University ‚Äì Penn‚Äôs neighboring university, located just to the north of campus.\n\nWould you like recommendations on things to see or do around campus?‚Äù\nI was pleasantly surprised by this answer, however, I think its level of knowledge is still less than a Upenn student‚Äôs. Specifically, if I were to ask a Penn student for landmarks around campus they probably wouldn‚Äôt mention ‚ÄúThe Palestra‚Äù and ‚ÄúSingh Center for Nanotechnology‚Äù nor would they mention any dorms/lecture halls. They would maybe mention something that is more intrinsic to Penn culture like Ben Frank. Also for nearby landmarks, I doubt a Penn student would say Drexel or 30th Street Station and maybe would answer Rittenhouse Square. Because of this, I am going to follow up and ask the following question:\nNow answer in the context of a Upenn student, what would you describe as 5 community staples of Penn Campus.\nChatgpt answered with the following:\n‚ÄúAs a Penn student, five community staples on campus that define the Penn experience would be:\n\nLocust Walk ‚Äì The heart of campus life, where students pass through daily to get to class, stop by club tables, or run into friends. During events like Spring Fling and Hey Day, it transforms into a hub of celebration.\nHouston Hall ‚Äì The country‚Äôs first student union and a go-to spot for students to grab food, study, or meet up with friends. It houses several dining options, study spaces, and is home to many student organization meetings.\nThe Quad (University of Pennsylvania College Houses) ‚Äì A defining first-year experience. Living in the Quad is almost a rite of passage, and the iconic red-brick buildings and central green spaces make it a central community hub for freshmen.\nVan Pelt Library ‚Äì The ultimate study spot with a space for every type of student, from the bustling first floor to the silence of the sixth. Students often joke about ‚Äúliving‚Äù here during finals season.\nWawa (38th & Spruce) ‚Äì More than just a convenience store, this Wawa is a beloved late-night food stop for Penn students. Whether it‚Äôs post-party hoagies, a quick coffee before class, or a mid-study snack run, this location is a true campus staple.\n\nEach of these places holds a special significance in student life, creating shared experiences that define the Penn community. Would you like recommendations for more niche or lesser-known spots?‚Äù\nWith this ‚ÄúPenn student‚Äù directive language, Chatgpt actually gave a really good answer. All of the locations that it mentioned are trademarks of the Upenn experience - particularly the iconic freshmen one. On top of that, I think knowing Quadwa is a nice cherry on top."
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#where-do-i-go",
    "href": "posts/003_ethan_bpost/Bpos3.html#where-do-i-go",
    "title": "Can ChatGPT Get me to class?",
    "section": "WHERE DO I GO???",
    "text": "WHERE DO I GO???\nNow that we have tested ChatGPTs ability to understand what is around Penn campus, we will test its ability to get ME around Penn Campus. Similar to last experiment we will start with an easy question and move up in dificulty. I started by asking ChatGPT:\n‚ÄúIf I am at Rodin college house what is the most optimal way to get to Houston Hall as a Upenn Student.‚Äù\nChatGPT gave the following response:\n‚ÄúIf you‚Äôre at Rodin College House, the most optimal way to get to Houston Hall as a Penn student would be:\n\nPrimary Route (Fastest & Most Direct) ‚Äì Approx. 7 min Walk\n\nExit Rodin and head south on 40th Street toward Locust Walk.\nTurn left (east) onto Locust Walk and continue straight.\nWalk along Locust Walk, passing Harrison College House, Gutmann College House, and the Quad.\nHouston Hall is on your left just before reaching 36th Street.\n\nThis route is the best because: - Locust Walk is pedestrian-friendly and the main thoroughfare of campus. - It‚Äôs a scenic route where you‚Äôll likely run into friends or student organizations tabling. - It‚Äôs well-lit and safe, making it a great option even at night.\n\n\nAlternative Route (Bad Weather or Indoor Option)\nIf it‚Äôs raining or too cold, you can modify your route slightly: 1. Walk through the high-rises (Harrison, Harnwell, or Gutmann) and cut over to Locust Walk via Gutmann‚Äôs indoor connector. 2. Continue on Locust Walk to Houston Hall.\nWould you like an optimized route for any other locations?‚Äù\nChatgpt messed up a couple of times in these directions. First, when you exit Rodin you can only go onto Locust Walk, meaning it‚Äôs not only unnecessary to go to 40th Street first, but it is impossible. On top of that, you would not pass Gutmann College house on the way. However, this situation might not be entirely ChatGPT‚Äôs fault; often when I plug in a college hall in Apple or Google Maps it will give me a more inefficient way of not using Locust - implying that Locust Walk might be the problem. I will ask ChatGPT how to get from 4040 Walnut Street to David Rittenhouse Laboratory to test that.\nChatGPT Responded with:\n‚ÄúTo get from 4040 Walnut Street to David Rittenhouse Laboratory (DRL) in the most optimal way as a Penn student, follow this route (~7 min walk):\n\n\nPrimary Route (Fastest & Most Direct)\n\nStart at 4040 Walnut Street and walk east (toward Center City) on Walnut Street.\nContinue straight on Walnut Street for four blocks, passing 40th St, 39th St, and 38th St.\nWhen you reach 33rd Street, turn left (north) onto 33rd Street.\nDavid Rittenhouse Laboratory (DRL) is immediately on your right, at 209 S 33rd St.\n\n\n\nAlternative Route (Scenic Campus Walk)\n\nWalk south on 40th Street toward Locust Walk.\nTurn left (east) onto Locust Walk and walk straight through campus.\nPass Gutmann College House, the Quad, and the Penn Bookstore.\nAt 33rd Street, turn left, and DRL will be straight ahead.\n\nWhich route is better? - The Walnut Street route is the fastest and most direct. - The Locust Walk route is more scenic and lets you walk through the heart of campus.\nLet me know if you need directions to another spot!‚Äù\nIts primary route is accurate, suggesting that Locust may have been the problem with the original experiment. However, when it suggested a scenic campus walk, it was correct about the directions but was wrong about passing the Penn Bookstore along the way. This suggests that it was able to figure out the exact directions but didn‚Äôt have the spatial awareness to understand the landscape on the way."
  },
  {
    "objectID": "posts/003_ethan_bpost/Bpos3.html#conclusion",
    "href": "posts/003_ethan_bpost/Bpos3.html#conclusion",
    "title": "Can ChatGPT Get me to class?",
    "section": "Conclusion",
    "text": "Conclusion\nChatGPT did a pretty solid job with this experiment. It was able to recognize major landmarks around Penn and even adapt its responses when prompted to think like a Penn student. While its initial answer included places like The Palestra and Singh Center, which a student probably wouldn‚Äôt think to mention, its second response nailed it with community staples like Houston Hall, Van Pelt, and Quadwa. That shows it can adjust its answers based on context, which is impressive.\nOn the navigation side, ChatGPT gave reasonable directions but had trouble with campus-specific details. The Rodin to Houston Hall route was a little off since it didn‚Äôt realize you can‚Äôt walk onto 40th Street directly from Rodin. But when asked about getting from 4040 Walnut to DRL, it mostly got the directions right, except for mistakenly mentioning passing the Penn Bookstore. This suggests it can process general routes well but struggles with spatial awareness of landmarks along the way.\nOverall, ChatGPT can recognize Penn landmarks and generate decent walking directions, but it‚Äôs still missing some of the instinctive knowledge a real Penn student would have. It seems to rely on external mapping data rather than lived experience, which leads to small but noticeable inaccuracies. While it‚Äôs good for general guidance, if you need the best route around campus, you‚Äôre still better off asking a fellow Penn student."
  }
]