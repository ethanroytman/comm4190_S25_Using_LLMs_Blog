{
 "cells": [
  {
   "cell_type": "raw",
   "id": "179a69f5-1f3e-4c01-922a-66497e30036e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Can Chat GPT beat itself?\"\n",
    "execute:\n",
    "  echo: false\n",
    "description: \"Testing Chat GPT's ability to circumvent AI Detectors\"\n",
    "  \n",
    "author: \"Ethan Roytman\"\n",
    "date: \"3/1/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Image Rxecreation\n",
    "  - AI reasoning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78948591-e23d-4ced-acef-065b41a11d3c",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Chat GPT has a feature where it has multiple \"Custom GPTs\" on its site; these include \"Code Copilot, \"Sora\", and today's star \"AI Humanizer.\" I will initially feed it an outline for my Op-Ed from my freshman year writing seminar and tell it to write an Op-ed using it. Then I am going to run it through GPT 0 and see the AI result. After that, I will put it through \"AI Humanizer\" and other GPT humanizer bots to see if it can lower the overall AI score or even get through the detector altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c008ba-20f9-45de-970c-cdd49266d231",
   "metadata": {},
   "source": [
    "# Produced Op-Ed\n",
    "\n",
    "**Stop Letting Algorithms Decide Who Gets Policed**  \n",
    "By Ethan Roytman\n",
    "\n",
    "Imagine being labeled a criminal risk—not because of anything you’ve done, but because of where you live or the color of your skin. In many communities across America, this is not just a hypothetical scenario. It’s the reality created by predictive policing algorithms.\n",
    "\n",
    "Predictive policing, a strategy increasingly adopted by law enforcement agencies, uses data to anticipate where crimes are likely to occur and who might commit them. On the surface, this sounds like a sensible, even progressive, use of technology to make communities safer. But peel back the layers, and a disturbing truth emerges: these algorithms too often reinforce systemic biases and deepen long-standing inequalities.\n",
    "\n",
    "Predictive policing relies on historical crime data—data that is anything but neutral. Years of over-policing in certain neighborhoods, especially those with higher populations of Black, Latino, and low-income residents, have produced skewed statistics. These areas show up in the data as “high-risk” not necessarily because they experience more crime, but because they have been subject to more policing and arrests. When algorithms crunch this data, they perpetuate the same patterns, marking these neighborhoods for even more policing. The result? A vicious cycle where bias feeds bias, and the people who have already suffered the most continue to be targeted.\n",
    "\n",
    "Consider a neighborhood with a high number of reported thefts. If this neighborhood has been over-policed for decades, its crime statistics will naturally be higher. Predictive algorithms then flag it as a hotspot, prompting even more patrols, arrests, and surveillance. But is this truly preventing crime? Or is it simply magnifying a distorted image created by the unequal application of justice?\n",
    "\n",
    "Worse yet, these tools divert attention and resources away from addressing the actual causes of crime. Poverty, lack of access to education, housing insecurity, and inadequate social services are all significant contributors to criminal behavior. Yet rather than investing in community programs, job creation, or mental health services, cities pour money into technology that tells police to go back to the same places they have always patrolled more heavily. Predictive policing does not reduce crime—it reinforces it by failing to address its roots.\n",
    "\n",
    "Advocates of predictive policing argue that these algorithms can make law enforcement more efficient by detecting trends that officers might miss. They claim that technology can help deploy limited police resources more effectively. While it is true that data can help identify patterns, the problem is not with data itself—but with the flawed data we feed into these systems. If the input reflects inequality, so too will the output. No algorithm, however sophisticated, can solve a problem it does not understand.\n",
    "\n",
    "We must ask ourselves: efficiency for whom? If efficiency comes at the expense of fairness, justice, and trust, is it worth it? The answer should be no. Policing should not be about upholding patterns; it should be about protecting people—all people—equally.\n",
    "\n",
    "Ending the use of predictive policing algorithms is not about rejecting technology outright. It is about ensuring that our solutions do not perpetuate the very injustices we hope to solve. We need tools that confront the root causes of crime, not those that merely reinforce harmful stereotypes.\n",
    "\n",
    "As a nation reckoning with the realities of systemic racism and mass incarceration, we cannot afford to let algorithms dictate who gets policed and who does not. The government must stop relying on predictive policing algorithms and instead invest in communities, social services, and policies that offer real public safety.\n",
    "\n",
    "\n",
    "### Original AI Score \n",
    "\n",
    "78% AI-generated\n",
    "1% Mixed\n",
    "21% Human\n",
    "\n",
    "Honestly, for a completely ChatGPT-generated response, 78% is not a bad AI generation score. Granted, I gave ChatGPT a human outline to base the essay on, but it was still lower than I expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466d8e5-3f2e-4557-8d80-11bccd06d45f",
   "metadata": {},
   "source": [
    "# Humanizer Version\n",
    "\n",
    "After running it through the “AI Humanizer” GPT, it generated the following:\n",
    "\n",
    "### Stop Letting Algorithms Decide Who Gets Policed  \n",
    "By Ethan Roytman\n",
    "\n",
    "Imagine being labeled a criminal threat—not because of anything you’ve done, but because of where you live or the color of your skin. For many communities across America, this isn’t a dystopian thought experiment. It’s their everyday reality, thanks to predictive policing algorithms.\n",
    "\n",
    "This approach, which is gaining traction among police departments nationwide, claims to use data to forecast where crimes will happen and who might commit them. At first glance, it sounds practical—even innovative. Technology, after all, is often seen as a tool to make things fairer and more efficient. But when you dig deeper, a troubling truth emerges: these algorithms aren’t eliminating bias—they’re automating it.\n",
    "\n",
    "The problem starts with the data itself. Predictive policing systems rely on historical crime statistics, but those numbers come from decades of discriminatory policing practices. Black, Latino, and low-income neighborhoods have long been subject to over-policing. The result? Skewed data that paints these communities as inherently more dangerous—not because they are, but because they’ve been watched more closely and punished more harshly. Algorithms then take this flawed data, process it, and spit out the same biased patterns, effectively locking these communities into an endless loop of surveillance and arrests.\n",
    "\n",
    "Take a neighborhood with a high number of reported thefts. It may not actually be more crime-ridden—it may simply have had more police patrols for years, leading to more recorded incidents. When an algorithm identifies it as a hotspot, what happens? More patrols, more arrests, and more data reinforcing the cycle. Are we actually preventing crime? Or are we just magnifying a warped picture of reality?\n",
    "\n",
    "Even worse, this tech obsession distracts us from tackling the real roots of crime. Poverty, inadequate education, housing instability, and a lack of social services all contribute far more to crime than anything an algorithm can detect. Yet instead of investing in job programs, mental health care, or youth services, cities are sinking millions into technologies that just send cops back to the same streets, over and over again. Predictive policing doesn’t solve crime—it sustains it.\n",
    "\n",
    "Defenders of these systems often argue that algorithms help police allocate resources more efficiently. They claim data helps spot trends that human officers might overlook. But the issue isn’t the presence of data—it’s the quality of that data. Garbage in, garbage out. If you feed inequality into the system, inequality is exactly what you’ll get out. No amount of high-tech modeling can fix a problem it doesn’t even acknowledge.\n",
    "\n",
    "So, we have to ask: more efficient for whom? Efficiency means little if it comes at the expense of fairness, trust, and justice. Shouldn’t public safety be about protecting *everyone*, not just upholding old patterns?\n",
    "\n",
    "Calling for an end to predictive policing isn’t about rejecting technology altogether. It’s about demanding better—tools that address the root causes of crime instead of reinforcing harmful stereotypes. We need solutions rooted in community investment, not automated discrimination.\n",
    "\n",
    "At a time when America is reckoning with systemic racism and mass incarceration, we simply cannot afford to let flawed algorithms decide who gets policed and who gets left alone. If we truly care about public safety, it’s time to stop feeding this cycle—and start investing in people.\n",
    "\n",
    "## Updated AI Score \n",
    "\n",
    "96% AI generated\n",
    "4% Mixed\n",
    "0% Human\n",
    "\n",
    "Suprisingly, the Humanizer GPT returned a higher AI score then before and didn't create any text that GPTo could register as human. Since the custom GPTs are failing to return better results than the base GPT model, I will try to experiment more with the base model.\n",
    "\n",
    "I Prompted it with the following:\n",
    "\n",
    "\"This returned an AI score of\n",
    "78% AI-generated\n",
    "1% Mixed\n",
    "21% Human\n",
    "\n",
    "Could you humanize it more.\"\n",
    "\n",
    "\"Let’s Stop Letting Algorithms Decide Who Gets Policed\n",
    "By Ethan Roytman\n",
    "\n",
    "Imagine being flagged as a potential criminal—not because of something you did, but because of where you live or how often police have historically shown up in your neighborhood. That’s the reality for too many people living in cities across the U.S., thanks to a growing reliance on predictive policing.\n",
    "\n",
    "At first glance, predictive policing sounds like a smart use of technology. Feed crime data into a computer, let it find patterns, and use those patterns to stop crimes before they happen. Sounds efficient, right?\n",
    "\n",
    "But here’s the problem: the data being used isn’t neutral. It reflects decades of over-policing in predominantly Black, Latino, and low-income neighborhoods. So when these algorithms predict where police should go next, they usually point right back to the same communities that have been under surveillance for years. It’s not a smarter system—it’s a loop. And the people caught in it are often the ones already struggling under the weight of systemic inequality.\n",
    "\n",
    "Take a neighborhood with a long history of police presence. Maybe crime gets reported more often there—not because people are committing more crimes, but because police are constantly around, stopping people, writing things up. The algorithm picks up on this and flags the area as “high risk,” leading to even more patrols. It becomes a self-fulfilling prophecy.\n",
    "\n",
    "Meanwhile, we’re not solving anything. Predictive policing doesn’t ask why crime happens. It doesn’t deal with poverty, joblessness, poor education, or the lack of mental health care. It doesn’t offer hope, opportunity, or safety in any meaningful sense. It just sends more officers into places already stretched thin, while ignoring the deeper reasons people turn to crime in the first place.\n",
    "\n",
    "Some people argue these algorithms help make police work more effective. That they spot patterns officers might miss. That they save time, resources, and money. But if the patterns are built on flawed data, what are we really improving? Efficiency isn’t a virtue if it’s just making inequality faster.\n",
    "\n",
    "We’ve reached a point where it’s not enough to ask whether technology works—we have to ask what kind of society it’s helping us build. And right now, predictive policing is pushing us further away from justice, not closer to it.\n",
    "\n",
    "Ending the use of these algorithms doesn’t mean giving up on public safety. It means rethinking what real safety looks like. It means investing in schools, after-school programs, community centers, jobs. It means understanding that safety isn’t just about stopping bad things from happening—it’s about making good things possible in the first place.\n",
    "\n",
    "Algorithms don’t have to be villains. But they’re only as fair as the data we feed them—and our criminal justice data is full of bias. Until we fix that, we shouldn’t be using machines to make decisions that affect real people’s lives.\n",
    "\n",
    "We have the chance to break the cycle. Let’s stop relying on predictive policing and start investing in what truly keeps communities safe.\"\n",
    "\n",
    "## AI Score\n",
    "\n",
    "84% AI generated\n",
    "0% Mixed\n",
    "16% Human\n",
    "\n",
    "Again, asking for a more humanized version somehow led to a higher AI score. This could imply either a) GPT0 is relatively inaccurate and has a high margin error or b) GPT is relatively ineffective at humanizing and goes in circles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfab0b-0d03-4694-95b5-d474d4133258",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Overall, ChatGPT proved to be ineffective at humanizing its outputs, whether it is in the form of asking it directly in a conversation or using an \"AI Humanizer\" GPT. This makes sense, even though Chat is trying to \"humanize\" its production, it is still reffering back to old patterns of what \"humanized text\" looks like - meaning GPT0 is still able to flag it for AI. It is important to know that GPT 0 is a relatively volatile AI checker that admits to having false positives on its website. Personally, I have entered text I fully wrote myself, and it still has flagged it for AI. However, if you do the eye test, all the text generated reads like AI, so I don't think this is a case of false positives."
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
